{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b5cea71a3770e00",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Maschinelles Lernen\n",
    "\n",
    "Nun kommen wir (endlich) zum eigentlichen Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87e228721ba72c2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import some necessary librairies\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt  # Matlab-style plotting\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "warnings.warn_explicit = ignore_warn\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n",
    "\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../data/house-prices-advanced-regression-techniques\"]).decode(\"utf8\")) #check the files available in the directory\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae33a41f465d866f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Daten aus dem vorigen Schritt laden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe73b86950116b2d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/house-prices-advanced-regression-techniques/x_preprocessed_train.pkl', 'rb') as handle:\n",
    "    X_preprocessed_train = pickle.load(handle)\n",
    "\n",
    "with open('../data/house-prices-advanced-regression-techniques/y_train.pkl', 'rb') as handle:\n",
    "    y_train = pickle.load(handle)\n",
    "    \n",
    "with open('../data/house-prices-advanced-regression-techniques/x_test.pkl', 'rb') as handle:\n",
    "    X_test = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69512506a4704850",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Verfahren maschinellen Lernens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705aaa23960b2ddf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Supervised Learning\n",
    "\n",
    "<br>\n",
    "<img src=\"../img/ml-overview.png\" width=\"70%\" align=\"center\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1109601add608ffe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Supervised Learning\n",
    "Der Bereich des Supervised Learnings (das überwachte maschinelle Lernen) lässt sich in drei wichti-ge Bereiche unterteilen: die Klassifikation, die Regression und die Segmentierung. Für die Klassifikation kann die logistische Regression eingesetzt werden. Vor allem die Support Vector Machine (SVM) und verschiedene Decision-Tree-Verfahren sind hier als Alternativen hervorzuheben. Tiefe neuronale Netze sind in diesem Bereich die wichtigsten Vertreter. Beim überwachten maschinellen Lernen werden Algorithmen verwendet, um ein Modell zu trainieren, das Muster in einem Datensatz mit Bezeichnungen und Merkmalen findet. Das trainierte Modell wird sodann verwendet, um die Bezeichnungen für die Merkmale eines neuen Datensatzes vorherzusagen. Entscheidend hierbei ist, dass für das Training Annotationen – d. h. manuelle Zuordnungen zwischen Eingangsdaten und Ausgangsdaten – vorliegen. Ist das Modell trainiert, dann kann für unbekannte Daten („New Data“) durch die Benut-zung des Modells ein Label vorhergesagt werden („Use Model“).\n",
    "\n",
    "<br>\n",
    "<img src=\"../img/supervised.png\" width=\"70%\" align=\"center\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174256e0822b7362",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Prognose, Verlust und Optimierung\n",
    "Bei vielen Verfahren des maschinellen Lernens ist das Ziel, ausgehend von bekannten Eingangsdaten eine Vorhersage über eine unbekannte Zielgröße zu treffen. Beim überwachten Lernen trainiert man ein Modell mit Eingangsdaten, für die der „richtige“ Wert der Zielgröße bekannt ist, um das Modell dahingehend zu optimieren, dass es auch für Eingangsdaten die Zielgröße vorhersagen kann, zu denen diese nicht bekannt ist. Wichtige Begriffe sind Prognose, Verlust und Optimierung. Ausgegangen wird von Eingangsdaten, die in der Abbildung mit (1) bezeichnet sind.\n",
    "Das können je nach Verfahren Bilder, Töne, numerische Werte, Texte oder andere Daten (z. B. Sensordaten aus dem Betrieb von Maschinen) sein. Für diese Daten ist die Zielgröße – auch als Y bezeichnet – bekannt, in der Abbildung als (2) bezeichnet. Im Beispiel geht es um eine Klassifizierung, d. h. die Entscheidung, welche Zahl eine handgeschriebene Ziffer darstellt. Man spricht davon, dass die Eingangsdaten – auch als X bezeichnet – einer bestimmten Klasse zugehören (hier sind dies die Klassen 0 bis 9). Man führt nun die Eingangsdaten dem Modell zu und berechnet mit Hilfe des Modells die Zielgröße. Dies wird als Prognose (3) bezeichnet und mit Y ̂ gekennzeichnet. An anderen Stellen ist auch vom Abschätzen oder einer Vorhersage die Rede. Dies wird für alle Trainingsdaten wiederholt. In Schritt (4) wird nun für alle Ergebnisse Y mit Y ̂ verglichen und die „Abweichung“ zwischen Y und Y ̂ für diesen Durchlauf ermittelt. Diese Abweichung bezeichnet man als Verlust. Hierfür kommt die Verlustfunktion zum Einsatz, mit deren Hilfe diese Abweichung berechnet werden kann. Das Ergebnis der Verlustfunktion wird nun genutzt, um die internen Parameter – die so genannten Modellparameter – derart zu verändern, dass im nächsten Durchlauf der Verlust verringert wird. Dieses Verfahren wird Optimierung genannt und in der Abbildung mit (5) bezeichnet. Mit Hilfe eine Optimierungsverfahrens, z. B. dem so genannten Gradientenabstieg, können nun die Modellparameter optimiert werden. Ergänzend sei erwähnt, dass es neben dem Gradientenabstieg viele weitere Optimierungsverfahren gibt, die unterschiedliche Eigenschaften, Vor- und Nachteile haben.\n",
    "\n",
    "<br>\n",
    "<img src=\"../img/prozess.png\" width=\"70%\" align=\"center\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c662cd1e6552bc89",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Regression\n",
    "\n",
    "## Interpolation\n",
    "\n",
    "<br>\n",
    "<img src=\"../img/interpolation.png\" width=\"70%\" align=\"center\">\n",
    "<br>\n",
    "\n",
    "## Extrapolation\n",
    "\n",
    "<img src=\"../img/extrapolation.png\" width=\"70%\" align=\"center\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29098cc9716d61",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Decision Tree\n",
    "<img src=\"../img/decisiontree.png\" width=\"70%\" align=\"center\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88df9c0ef6237253",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Gradient Boosted Decision Trees (GBDT)\n",
    "\n",
    "Bei GBDT – Gradient Boosted Decision Trees – werden mehrere solcher Entscheidungsbäume kom-biniert. Die Idee dahinter ist, dass über ein Gradient-Descent-Verfahren der Verlust mehrerer für sich genommen möglicherweise schwacher Decision-Tree-Modelle insgesamt der Verlust minimiert werden kann.\n",
    "\n",
    "Beim Gradient Boosting werden nicht mehr spezielle Datensätze besonders (durch die Gewichte) berücksichtigt, sondern es wird der Gradient zwischen den korrekt vorhergesagten (klassifizierten, berechneten) Mustern und den Antworten der vorherigen Hypothese betrachtet. Für die Optimierung, dem Gradientenabstieg, wird eine differenzierbare Verlustfunktion benötigt; das kann für eine Klassifizierung der „squared error“ und für die Regression der „logarithmische Verlust“ sein. Anstatt die Parameter des Modells mit Hilfe des Gradientenabstiegs zu optimieren, werden – in Richtung der stärksten Verringerung des Gradienten – die Parameter der nächsten Hypothese, also des nächsten Modells, angepasst. \n",
    "\n",
    "**XGBoost**\n",
    "\n",
    "Der XGBoost-Algorithmus ist eine Implementierung des GBDT für hochparallele Verarbeitung z. B. auf GPU-Prozessoren, die gegenüber CPU-Prozessoren sehr viel mehr Recheneinheiten zur Verfügung haben, die derartige Optimierungsprozesse – also Lernprozesse – parallelisieren und damit die Geschwindigkeit des Trainings steigern können. XGBoost ist eine Open-Source-Implementierung, die Gradient Boosting zusammen mit Pruning und Regularisierung einsetzt.\n",
    "\n",
    "<img src=\"../img/gbdt.png\" width=\"70%\" align=\"center\">\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed25d24443583de9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Laden der Daten und Aufteilung in Train und Test Daten\n",
    "\n",
    "Um die Güte eines trainierten Modells zu testen, werden sogenannte Test-Daten zurückgehalen. Diese Daten hat das Modell noch nie gesehen. So kann gemessen werden, wie gut das Netz generalisiert, also mit unbekannten Daten umgehen kann.\n",
    "\n",
    "**Cross Validation**\n",
    "Mit der n-fachen Kreuzvalidierung werden innerhalb der Validierungsdaten nun zwei Subsets definiert: Das Trainings-Set und das Validieruns-Set. Dieses wird über den gesamten Traingings-Datenbestand mutiert.\n",
    "\n",
    "<img src=\"../img/test_train_split.png\" width=\"70%\" align=\"center\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa6bd94fb1b805",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "seed = 4354 # Random_state ist ein seed, damit gegebenenfalls immer mit der selben pseudo Random Folge gearbeitet wird.\n",
    "X_train, X_validate, y_train, y_test = train_test_split(X_preprocessed_train, y_train, test_size=0.2, random_state=seed) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82cd41f1c844991",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Grid-Search\n",
    "Machine Learning Modelle benötigen in der Regel Hyperparameter, d.h. Steurungsparameter, die außerhalb der eigenentlichen internen (angelernten) Modell-Parameter liegen.\n",
    "Diese können über eine systematische Suche gefunden werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd720f3012f98e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Wir wollen drei Modelle untercuchen\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'RandomForest': RandomForestRegressor(random_state=seed),\n",
    "    'XGBoost': XGBRegressor(random_state=seed)\n",
    "}\n",
    "\n",
    "# Define the hyperparameter grids for each model\n",
    "param_grids = {\n",
    "    'LinearRegression': {},\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'max_depth': [None, 10, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'max_depth': [3, 6, 10],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14496a0b2dbb5495",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Festlegung der Kreuzvalidierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c631ad1b90cd5358",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3-fold cross-validation\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989bb9d061a991cc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training, Gridsearch und Kreuzvalidierung\n",
    "\n",
    "Training, Gridsearch und Kreuzvalidierung können dank der vorbereiteten Funktion in scikit-learn einfach ausgeführt werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e01afd80f05dc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train and tune the models\n",
    "grids = {}\n",
    "for model_name, model in models.items():\n",
    "    #print(f'Training and tuning {model_name}...')\n",
    "    grids[model_name] = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "    grids[model_name].fit(X_train, y_train)\n",
    "    best_params = grids[model_name].best_params_\n",
    "    best_score = np.sqrt(-1 * grids[model_name].best_score_)\n",
    "    \n",
    "    print(f'Optimale Parameter für {model_name}: {best_params}')\n",
    "    print(f'Bester RMSE Score für {model_name}: {best_score}\\n') #Root-mean-square deviation\n",
    "    \n",
    "# Vgl. https://www.kaggle.com/code/kenjee/housing-prices-example-with-video-walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b752214a2854b8d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
